{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인공지능(AI)\n",
    "\n",
    "소프트웨어측면 - 인공지능(AI)  \n",
    "하드웨어측면 - 양자이론, 양자컴퓨터\n",
    "\n",
    "- CS에서 궁극의 목표 중 하나\n",
    "- 문제도 많아\n",
    "    - 일자리 감소\n",
    "    - 20년 뒤에는 직업의 절반가약이 없어지고 거기서 10년정도 더지나면 모든직업이 사라진다는 예측이 있다\n",
    "    - 사람은 무엇을 하며 살아야 하는가?\n",
    "\n",
    "\n",
    "- 빅뱅의 시작을 1년으로 잡으면 \n",
    "- 인류의 탄생은 2일전\n",
    "- 산업혁명은 2초전\n",
    "- 기술의 발전속도는 기하급수족으로 증가하고 있다\n",
    "- 언젠가는 우리가 만드는 프로그램이 사람의 지능을 앞서는 순간이 올거라고 예측할 수 있다\n",
    "- 이 시점을 특이점(singlurarity)라고 한다\n",
    "- 조만간 특이점이 올꺼라고 많은 사람들이 생각하고 있고 ,\n",
    "- 그 시점을 사람들이 에측해보건데,,,2045년도에 아마도 탄생!\n",
    "- 많은 학자들 중 일부는 특이점이 오는 시기가 인류가 멸망하는 시기라고 얘기한다.\n",
    "- 회사들은 안전..!강조\n",
    "    - AI가 선악을 구별할 수 있을것이다...==> 하지만 행동은 일치하지 않을 수도 있어\n",
    "    - 프로그래밍을 통해서 AI를 제어할 수있다,..!\n",
    "        - 뇌과학자 => AI가 개발이되면 인공지능은 전자회로의 속도로 학습을 하고 사람은 생화학적 회로로 학습\n",
    "        - 전자회의 속도가 약 100만배정도 빠르다\n",
    "        - MIT의 AI개발자들이 만약 인공지능 만들어내면, 인공지능이 1주일 동안 할 수 있을 MIT AI개발자들이 2만년\n",
    "        - 지능의 차이가 너무 크기 때문애\n",
    "        - 인공지능이 사람을 미개하게 봄\n",
    "        - 프로그래밍으로 제어 불가능\n",
    "\n",
    "### 양자컴퓨터\n",
    "\n",
    "- 현시점에 가장 빠른 슈퍼컴퓨터가 미국 - IBM이 만든 서밋(summit)\n",
    "    - 농구코드 2배정도되는 크기에 캐비넷 깔아놓고 그 안에 컴퓨터 채워넣음\n",
    "    - 기후 예측\n",
    "    - 화성 탐사선 우주개발 시뮬레이션\n",
    "    \n",
    "\n",
    "- 작년 10월 23일 구글이 네이처지에 양자컴퓨터 개발에 대한 내용을 발표\n",
    "- 서밋이 1000년동안 해야하는 일을 3.5초만에 해결\n",
    "- 2.5일 정도 되는 일을 3.5초에 해결\n",
    "- 지금현재는 양자 컴퓨터는 특정분야에만 적용가능 ( 인강의 게놈지도, 양자연구 등 )\n",
    "- 일반적인 computing은 사용할 수 없어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인공지능(AI)\n",
    "- 궁극의 목표 중 하나\n",
    "- 1960년대부터 꾸준히 연구, 개발\n",
    "\n",
    "\n",
    "- 인간의 뇌를 연구하기 시작\n",
    "- 1958년에 perceptron을 모델링한 기계를 실제로 구현\n",
    "- 뉴욕타임즈에 기사가 실림\n",
    "- \"조금만 있으면 스스로 말하고, 듣고, 쓰고, 창조가 가능한 프로그램을 만들 수 있다\"\n",
    "\n",
    "\n",
    "### AND/OR에 대한 logistic regression => perceptron\n",
    "- 진리표를 이용해서 학습할 것이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost값은 : 0.7289631366729736\n",
      "Cost값은 : 0.7184419631958008\n",
      "Cost값은 : 0.7127659320831299\n",
      "Cost값은 : 0.7085617780685425\n",
      "Cost값은 : 0.7052786350250244\n",
      "Cost값은 : 0.7026972770690918\n",
      "Cost값은 : 0.7006663680076599\n",
      "Cost값은 : 0.6990684270858765\n",
      "Cost값은 : 0.6978110074996948\n",
      "Cost값은 : 0.6968214511871338\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[0,0],\n",
    "          [0,1],\n",
    "          [1,0],\n",
    "          [1,1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight, bias\n",
    "W = tf.Variable(tf.random_normal([2,1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# Cost\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                              labels=Y))\n",
    "\n",
    "# train\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# session, 초기화\n",
    "sess= tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# nodes\n",
    "for step in range(3000): #  코스트값을 3000번 줄이겠다 는 뜻\n",
    "    _, cost_val=sess.run([train, cost], feed_dict = {X : x_data,\n",
    "                                                     Y : y_data} ) # 먹이를 줘야지 그데이터가지고 학습해 \n",
    "    if step % 300 == 0:\n",
    "        print(\"Cost값은 : {}\".format(cost_val))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perceptron 만들었어\n",
    "# 이제 잘 돌아가는지 prediction해줘야해\n",
    "\n",
    "predict = tf.cast(H > 0.5, dtype = tf.float32)\n",
    "sess.run(predict, feed_dict={X : [[0,0]]})\n",
    "\n",
    "# => perceptron이 학습해서 AND gate, OR gate 를 만들수 있군!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron(logistic)\n",
    "- perceptron으로 AND/OR는 구현이 가능\n",
    "- XOR(Exclusive OR)은 perceptron으로 구현이 안돼\n",
    "- 많은 사람들이 XOR을 어떻게 Perceptron으로 구현할 수 있을까\n",
    "- gate의 집합으로 칩을 만들어 -> 칩이 학습이 가능 -> 인공지능\n",
    "\n",
    "\n",
    "#### 1969년 마빈 민스키라는 사람이 논문을 하나 발표\n",
    "   - MIT AI lab 창시자\n",
    "   - XOR은 한개의 Perceptron으로 학습이 불가능!\n",
    "   - MLP(Multi Layor Perceptron)으로는 가능해!\n",
    "   - MLP는 학습이 너무 어려워서 지구상에 있는 누구라도 이 학습을 시킬 수가 없어\n",
    "   - 학습할 수 있는 방법을 찾기가 힘들어!  \n",
    "=> AI가 망함\n",
    "\n",
    "\n",
    "\n",
    "#### 1974년도 Paul 이라는 박사과정 학생이 Backpropapation방법을 고안\n",
    "- 그러나 이미 배는 떠났어\n",
    "\n",
    "\n",
    "\n",
    "#### 1982년도에 다시한번 논문을 발표!\n",
    "\n",
    "\n",
    "\n",
    "#### 1986년도에 Hinton교수님이 논문을 발표!  \n",
    "- 활활 타올르게됨!! AI가 각광을 받기 시작\n",
    "\n",
    "\n",
    "#### 1995년쯤에 BackPropagation방식이 안되는건 아닌데 복잡한 문제는 역시나 해결이 불가능해\n",
    "   - 이 시기에 다른 여러가지 알고리즘이 나타나기 시작\n",
    "   - SVM, 나이브 베이지안, Decision Tree...\n",
    "   - LeCUN => 다른 알고리즘이 더 우수하다는 것을 증명  \n",
    "다시 침체기...\n",
    "\n",
    "\n",
    "\n",
    "#### Canadian Institute For Advanced Research(CIFAR)  \n",
    "- 캐나다가 국책 연구기관을 설립  \n",
    "#### 1987년에 Hinton교수 캐나다로 건너가서 AI연구를 지속\n",
    "\n",
    "\n",
    "#### 2006,2007년도에 2개의 논문을 발표  \n",
    "- 망했던 이유 찾았어!\n",
    "\n",
    "- 2006년 => w,b의 초기값을 random으로 주면 안돼!  \n",
    "- 2007년 => 초기값에 대한 증명에 대한 논문, layer를 더 많이 사용할 수록 복잡한 문제를 해결할 수 있어  \n",
    "\n",
    "#### 사람들의 반응이 차디차...\n",
    "### 신분세탁...rebranding => Deep Learning\n",
    "  \n",
    "  \n",
    "## XOR 하나의 logistic으로 안돼 => 여러가지 perceptron으로! => deeplearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Cost값은 : 4.295819282531738\n",
      "Cost값은 : 0.13018810749053955\n",
      "Cost값은 : 0.03531719371676445\n",
      "Cost값은 : 0.017436044290661812\n",
      "Cost값은 : 0.010974248871207237\n",
      "Cost값은 : 0.007806999608874321\n",
      "Cost값은 : 0.005971984006464481\n",
      "Cost값은 : 0.004791435785591602\n",
      "Cost값은 : 0.003975639119744301\n",
      "Cost값은 : 0.0033818904776126146\n"
     ]
    }
   ],
   "source": [
    "## logistic 갯수가 2개\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[0,0],\n",
    "          [0,1],\n",
    "          [1,0],\n",
    "          [1,1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight, bias\n",
    "W1 = tf.Variable(tf.random_normal([2,10]), name=\"weight1\")\n",
    "# W1 = tf.Variable(tf.random_normal([2,logistic에 들어갈 x 갯수]), name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([10]), name=\"bias\")\n",
    "# b1 = tf.Variable(tf.random_normal([logistic에 들어갈 x 갯수]]), name=\"bias\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10,256]), name=\"weight2\")\n",
    "# W2 = tf.Variable(tf.random_normal([logistic에 들어갈 x 갯수,1]), name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([256]), name=\"bias2\")\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256,1]), name=\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([1]), name=\"bias3\")\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(layer2,W3) + b3\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# Cost\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                              labels=Y))\n",
    "\n",
    "# train\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# session, 초기화\n",
    "sess= tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# nodes\n",
    "for step in range(3000): #  코스트값을 3000번 줄이겠다 는 뜻\n",
    "    _, cost_val=sess.run([train, cost], feed_dict = {X : x_data,\n",
    "                                                     Y : y_data} ) # 먹이를 줘야지 그데이터가지고 학습해 \n",
    "    if step % 300 == 0:\n",
    "        print(\"Cost값은 : {}\".format(cost_val))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = tf.cast(H > 0.5, dtype = tf.float32)\n",
    "sess.run(predict, feed_dict={X : [[1,1]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0113\n",
    "\n",
    "---\n",
    "# MNIST with Neural Network(Deep Learning)\n",
    "\n",
    "- tensorflow가 기본으로 제공하는 예제를 이용해서 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")  # warning 출력 X\n",
    "\n",
    "# Data Loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)    # 예제라 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:1.6087576150894165\n",
      "cost:0.3924584984779358\n",
      "cost:0.4583384394645691\n",
      "cost:0.23724912106990814\n",
      "cost:0.3529265522956848\n",
      "cost:0.26094135642051697\n",
      "cost:0.14467860758304596\n",
      "cost:0.17265743017196655\n",
      "cost:0.1447421908378601\n",
      "cost:0.09782430529594421\n"
     ]
    }
   ],
   "source": [
    "# placeholder\n",
    "X = tf.placeholder(shape = [None,784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape = [None,10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias(Deep & Wide)\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]), name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "# depth가 이깊으로면깊을수록 연상이 안돼\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    " \n",
    "W2 = tf.Variable(tf.random_normal([256, 256]), name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([256]), name=\"bias2\")\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    " \n",
    "W3 = tf.Variable(tf.random_normal([256, 10]), name=\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([10]), name=\"bias3\")\n",
    "# 사실 sigmoid쓰나 softmax쓰나 같은결과, softmax는 전체 확률로 바뀌고 sigmoid에서 가장 큰 값이 softmax에서도 가장 큰값인건 바뀌지 않음\n",
    "\n",
    "\n",
    "# Hypothesis\n",
    "logit= tf.matmul(layer2,W3) + b3\n",
    "H = tf.sigmoid(logit)\n",
    "# sigmoid를 해서 softmax하면 가장 효율적임#\n",
    "\n",
    "# cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# session chrlghk\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "num_of_epoch = 30   #반복횟수\n",
    "batch_size = 100\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)\n",
    "    cost_val=0\n",
    "                      \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x,batch_y = mnist.train.next_batch(batch_size)   # x,y 100개씩 떼오기\n",
    "        _,cost_val = sess.run([train,cost], feed_dict = {X:batch_x, \n",
    "                                                         Y:batch_y})\n",
    "    if step%3 ==0:\n",
    "        print(\"cost:{}\".format(cost_val))\n",
    "                 \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.9218999743461609\n"
     ]
    }
   ],
   "source": [
    "# Accuracy 측정\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "print(\"정확도 : {}\".format(sess.run(accuracy, \n",
    "                               feed_dict={X:mnist.test.images,\n",
    "                                          Y:mnist.test.labels})))\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 생각보다 정확도가 많이 향상되지 않았어\n",
    "- Hinton 그 원인을 파악하려고 노력!\n",
    "- deep learning은 조금 더 학습이 잘 되기 위해서 layer을 추가하고 각 layer에 많은 perceptron을 추가해서 구현\n",
    "- 초기에 w값을 random으로 줬더니 값이 개판이네?\n",
    "- 초기값은 이렇게 주면안돼 \n",
    "- 식을 찾아냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:26.487985610961914\n",
      "cost:3.0200021266937256\n",
      "cost:3.1124086380004883\n",
      "cost:1.157516598701477\n",
      "cost:0.9435765147209167\n",
      "cost:0.6141703128814697\n",
      "cost:0.08506942540407181\n",
      "cost:0.4571163058280945\n",
      "cost:0.00012241600779816508\n",
      "cost:0.05606520175933838\n"
     ]
    }
   ],
   "source": [
    "# placeholder\n",
    "X = tf.placeholder(shape = [None,784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape = [None,10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias(Deep & Wide)\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]), name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "# depth가 이깊으로면깊을수록 연상이 안돼\n",
    "layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    " \n",
    "W2 = tf.Variable(tf.random_normal([256, 256]), name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([256]), name=\"bias2\")\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    " \n",
    "W3 = tf.Variable(tf.random_normal([256, 10]), name=\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([10]), name=\"bias3\")\n",
    "# 사실 sigmoid쓰나 softmax쓰나 같은결과, softmax는 전체 확률로 바뀌고 sigmoid에서 가장 큰 값이 softmax에서도 가장 큰값인건 바뀌지 않음\n",
    "\n",
    "\n",
    "# Hypothesis\n",
    "logit= tf.matmul(layer2,W3) + b3\n",
    "H = tf.nn.relu(logit)\n",
    "# sigmoid를 해서 softmax하면 가장 효율적임#\n",
    "\n",
    "# cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y)) # 이건 그대로\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session chrlghk\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "num_of_epoch = 30   #반복횟수\n",
    "batch_size = 100\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)\n",
    "    cost_val=0\n",
    "                      \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x,batch_y = mnist.train.next_batch(batch_size)   # x,y 100개씩 떼오기\n",
    "        _,cost_val = sess.run([train,cost], feed_dict = {X:batch_x, \n",
    "                                                         Y:batch_y})\n",
    "    if step%3 ==0:\n",
    "        print(\"cost:{}\".format(cost_val))\n",
    "                 \n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.9325000047683716\n"
     ]
    }
   ],
   "source": [
    "# Accuracy 측정\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "print(\"정확도 : {}\".format(sess.run(accuracy, \n",
    "                               feed_dict={X:mnist.test.images,\n",
    "                                          Y:mnist.test.labels})))\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinton 교수님이 중요하게 여기는 또하나의 요건은 W의 초기값!\n",
    "- 초기에는 RBM이라는 방법을 이용해서 초기화를 진행!\n",
    "- 2010년도 Xavier 초기화라는 방식이 논문으로 발표!  \n",
    "\n",
    "\n",
    "- 2015년도 He's 초기화라는 방식이 논물으로 발표\n",
    "- 현재도 계속 연구가 진행되고 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W1 = tf.Variable(tf.random_normal([784,256]), name=\"weight1\")\n",
    "W1 = tf.get_variable(\"weight1\", shape=[784,256],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())  # 초기화는 자비에르 초기화를 이용해서 w를 정해줘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:0.34568750858306885\n",
      "cost:0.03377196192741394\n",
      "cost:0.03904537111520767\n",
      "cost:0.008778831921517849\n",
      "cost:0.009305762127041817\n",
      "cost:0.002068533096462488\n",
      "cost:0.002165293786674738\n",
      "cost:0.0038126418367028236\n",
      "cost:0.00407616188749671\n",
      "cost:0.0025931678246706724\n"
     ]
    }
   ],
   "source": [
    "# 그래프 초기화\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape = [None,784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape = [None,10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias(Deep & Wide)\n",
    "W1 = tf.get_variable(\"weight1\", shape=[784,256],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "# depth가 이깊으로면깊을수록 연상이 안돼\n",
    "layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    " \n",
    "W2 = tf.get_variable(\"weight2\", shape=[256,256],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]), name=\"bias2\")\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    " \n",
    "W3 = tf.get_variable(\"weight3\", shape=[256,10],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name=\"bias3\")\n",
    "# 사실 sigmoid쓰나 softmax쓰나 같은결과, softmax는 전체 확률로 바뀌고 sigmoid에서 가장 큰 값이 softmax에서도 가장 큰값인건 바뀌지 않음\n",
    "\n",
    "\n",
    "# Hypothesis\n",
    "logit= tf.matmul(layer2,W3) + b3\n",
    "H = tf.nn.relu(logit)\n",
    "# sigmoid를 해서 softmax하면 가장 효율적임#\n",
    "\n",
    "# cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y)) # 이건 그대로\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# session chrlghk\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "num_of_epoch = 50   #반복횟수\n",
    "batch_size = 100\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)\n",
    "    cost_val=0\n",
    "                      \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x,batch_y = mnist.train.next_batch(batch_size)   # x,y 100개씩 떼오기\n",
    "        _,cost_val = sess.run([train,cost], feed_dict = {X:batch_x, \n",
    "                                                         Y:batch_y})\n",
    "    if step%5 ==0:\n",
    "        print(\"cost:{}\".format(cost_val))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.98089998960495\n"
     ]
    }
   ],
   "source": [
    "# Accuracy 측정\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "print(\"정확도 : {}\".format(sess.run(accuracy, \n",
    "                               feed_dict={X:mnist.test.images,\n",
    "                                          Y:mnist.test.labels})))\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ed565e4c6b75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# MinMax scaler가 min, max값 가지고 있다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprediction_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#sess.run(H,feed_dict={X:prediction_data})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scaler' is not defined"
     ]
    }
   ],
   "source": [
    "## 진짜 test 파일로 돌려보자!!\n",
    "import pandas as pd\n",
    "test_data = pd.read_csv(\"./data/digit-recognizer/test.csv\")\n",
    "\n",
    "# MinMax scaler가 min, max값 가지고 있다.\n",
    "prediction_data = scaler.transform(test_data)\n",
    "\n",
    "#sess.run(H,feed_dict={X:prediction_data})\n",
    "result = sess.run(tf.argmax(H,1), feed_dict={X:prediction_data})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = pd.DataFrame()\n",
    "my_df[\"ImageId\"] = range(1,test_data.shape[0]+1)\n",
    "my_df[\"Label\"] = result\n",
    "my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.to_csv(\"mnist_submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:0.12687115371227264\n",
      "cost:0.21654219925403595\n",
      "cost:0.0023357481695711613\n",
      "cost:0.08888078480958939\n",
      "cost:0.016398213803768158\n",
      "cost:0.037754759192466736\n",
      "cost:0.05694064497947693\n",
      "cost:0.11352460086345673\n",
      "cost:0.049056585878133774\n",
      "cost:0.01743735559284687\n"
     ]
    }
   ],
   "source": [
    "# 그래프 초기화\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape = [None,784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape = [None,10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias(Deep & Wide)\n",
    "W1 = tf.get_variable(\"weight1\", shape=[784,256],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "# depth가 이깊으로면깊을수록 연상이 안돼\n",
    "layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    " \n",
    "W2 = tf.get_variable(\"weight2\", shape=[256,256],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]), name=\"bias2\")\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    " \n",
    "W3 = tf.get_variable(\"weight3\", shape=[256,10],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name=\"bias3\")\n",
    "# 사실 sigmoid쓰나 softmax쓰나 같은결과, softmax는 전체 확률로 바뀌고 sigmoid에서 가장 큰 값이 softmax에서도 가장 큰값인건 바뀌지 않음\n",
    "\n",
    "\n",
    "# Hypothesis\n",
    "logit= tf.matmul(layer2,W3) + b3\n",
    "H = tf.nn.relu(logit)\n",
    "# sigmoid를 해서 softmax하면 가장 효율적임#\n",
    "\n",
    "# cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y)) # 이건 그대로\n",
    "\n",
    "# train\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)    # 이것도 자주쓰이지만 큰 효과는 x\n",
    "\n",
    "# session chrlghk\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "num_of_epoch = 50   #반복횟수\n",
    "batch_size = 100\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)\n",
    "    cost_val=0\n",
    "                      \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x,batch_y = mnist.train.next_batch(batch_size)   # x,y 100개씩 떼오기\n",
    "        _,cost_val = sess.run([train,cost], feed_dict = {X:batch_x, \n",
    "                                                         Y:batch_y})\n",
    "    if step%5 ==0:\n",
    "        print(\"cost:{}\".format(cost_val))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.9724000096321106\n"
     ]
    }
   ],
   "source": [
    "# Accuracy 측정\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "print(\"정확도 : {}\".format(sess.run(accuracy, \n",
    "                               feed_dict={X:mnist.test.images,\n",
    "                                          Y:mnist.test.labels})))\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting(과적합)\n",
    "- 학습한 모델이 training data set에 최적화되어 있는 상태\n",
    "- 테스트 데이터에는 잘 들어맞지 않는 상태를 지칭\n",
    "\n",
    "\n",
    "- 학습한 모델이 training data set에는 약 98% 이상 정확도를 가지는 test data set에 대해서는 85%정도 수준으로 정확도가 나오면 overfitting\n",
    "\n",
    "\n",
    "1. 일단 학습하는 데이터 수가 많아야 해\n",
    "2. 필요없는 feature들은 학습에서 제외!  \n",
    "   중복되는 feature들은 단일화 시켜야 해!\n",
    "3. 학습하는 과정에서 overfitting을 피할 수 있다  \n",
    "   2014년도 논문이 나와!  \n",
    "   dropout이라고 말해요!  \n",
    "   함수형태로 제공한다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:0.34584560990333557\n",
      "cost:0.09328232705593109\n",
      "cost:0.2504459321498871\n",
      "cost:0.25693222880363464\n",
      "cost:0.2198241800069809\n",
      "cost:0.139633908867836\n",
      "cost:0.11251033842563629\n",
      "cost:0.11583717167377472\n",
      "cost:0.3300926685333252\n",
      "cost:0.13570378720760345\n"
     ]
    }
   ],
   "source": [
    "# 그래프 초기화\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape = [None,784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape = [None,10], dtype=tf.float32)\n",
    "dout_rate = tf.placeholder(dtype=tf.float32)   # sclar이기 때문에 shape안적어줌\n",
    "\n",
    "# Weight & bias(Deep & Wide)\n",
    "W1 = tf.get_variable(\"weight1\", shape=[784,256],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "# depth가 이깊으로면깊을수록 연상이 안돼\n",
    "_layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "layer1 = tf.nn.dropout(_layer1, rate=dout_rate)   # 256개 output을 다 뽑아내지 않겠다. node를 아예삭제하는게 아니라 \n",
    "# 기능을 상실키시는것\n",
    "# rate=0이 다 살아있는거\n",
    "# rate=0.3 => 30% 죽인 layer\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\", shape=[256,256],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]), name=\"bias2\")\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "layer2 = tf.nn.dropout(_layer2, rate=dout_rate) \n",
    "    \n",
    "W3 = tf.get_variable(\"weight3\", shape=[256,10],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name=\"bias3\")\n",
    "# 사실 sigmoid쓰나 softmax쓰나 같은결과, softmax는 전체 확률로 바뀌고 sigmoid에서 가장 큰 값이 softmax에서도\n",
    "# 가장 큰값인건 바뀌지 않음\n",
    "\n",
    "\n",
    "# Hypothesis\n",
    "logit= tf.matmul(layer2,W3) + b3\n",
    "H = tf.nn.relu(logit)\n",
    "# sigmoid를 해서 softmax하면 가장 효율적임#\n",
    "\n",
    "# cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y)) # 이건 그대로\n",
    "\n",
    "# train\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)    # 이것도 자주쓰이지만 큰 효과는 x\n",
    "\n",
    "# session chrlghk\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "num_of_epoch = 50   #반복횟수\n",
    "batch_size = 100\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)\n",
    "    cost_val=0\n",
    "                      \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x,batch_y = mnist.train.next_batch(batch_size)   # x,y 100개씩 떼오기\n",
    "        _,cost_val = sess.run([train,cost], feed_dict = {X:batch_x, \n",
    "                                                         Y:batch_y,\n",
    "                                                         dout_rate:0.3})   # 30%끄고 학습 overfitting 피하려고\n",
    "    if step%5 ==0:\n",
    "        print(\"cost:{}\".format(cost_val))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.9606000185012817\n"
     ]
    }
   ],
   "source": [
    "# Accuracy 측정\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "print(\"정확도 : {}\".format(sess.run(accuracy, \n",
    "                               feed_dict={X:mnist.test.images,\n",
    "                                          Y:mnist.test.labels,\n",
    "                                          dout_rate:0})))\n",
    "# 학습은 overfitting 피하려고 30% 죽이고 하는데 정확도는 0으로 해줘야해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU_ENV]",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
