{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# 분류(classification)  #################\n",
    "예측하려는 대상의 속성(설명 변수)을 입력 받고, 목표 변수가 갖고 있는 카테고리(범주형)값 중에서 어느 한 값으로 분류하여 예측한다.\n",
    "고객 분류, 질병 진단, 스팸 메일 필터링, 음성 인식 등 목표 변수가 카테고리 값(범주형)을 갖는 경우에 사용한다.\n",
    "KNN, SVM, Decision Tree, Logistic Regression등 \n",
    "\n",
    "Logistic Regression는 종속변수(Y), 독립변수(X)간의 관계를 나타내는 예측모델이면서 목표 변수가 카테고리 값(범주형)을 갖는 경우에는 분류 분석에 해당된다\n",
    "종속변수(Y)에 로릿변환을 수행 => 로지스틱 회귀 분석\n",
    "로지스틱 회귀 모델은 독립변수(X) 값에 관계 없이 종속변수(Y)의 값은 0-1 사이에 있다\n",
    "\n",
    "#statsmodel 패키지의 로지스틱 회귀 분류 분석\n",
    "from sklearn.datasets import make_classification \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "X, y=make_classification(n_features=1,  n_redundant=0, n_informative=1, \n",
    "n_clusters_per_class=1, random_state=4)\n",
    "\n",
    "plt.scatter(X, y, s=100, edgecolor=\"k\", linewidth=2)\n",
    "sns.distplot(X[y==0, :], label=\"y=0\", hist=False)\n",
    "sns.distplot(X[y==1, :], label=\"y=1\", hist=False)\n",
    "plt.ylim(-0.2, 1.2)\n",
    "plt.show()\n",
    "\n",
    "#statsmodel 패키지에서 베르누이 분포를 따르는 로지스틱 회귀 모델 클래스 Logit 제공\n",
    "\n",
    "import statsmodels.api as sm\n",
    "x = sm.add_constant(X)\n",
    "logit_model = sm.Logit(y, x)\n",
    "logit_result = logit_model.fit(disp=0)\n",
    "print(logit_result.summary())   # (4.2382x+0.2515)  \n",
    "\n",
    "import numpy as np\n",
    "xx = np.linspace(-3, 3, 100)\n",
    "mu = logit_result.predict(sm.add_constant(xx))\n",
    "plt.plot(xx, mu, lw=3)\n",
    "plt.scatter(X, y, c=y, edgecolor=\"k\", lw=2)\n",
    "plt.scatter(X, logit_result.predict(x), label=r\"$\\hat{y}$\", marker='s', c=y, s=100, edgecolor=\"k\", lw=1)\n",
    "plt.xlim(-3, 3)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(r\"$\\mu$\")\n",
    "plt.title(r\"$\\hat{y}$\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#다중 클래스의 로지스틱 회귀 분석\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = datasets.load_iris()   \t\t# 데이터 로드\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "scaler = StandardScaler()   \t\t# 특성을 표준화\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "# OVR 로지스틱 회귀 모델을 만듭니다.\n",
    "logistic_regression = LogisticRegression(random_state=0, multi_class=\"ovr\")\n",
    "model = logistic_regression.fit(features_standardized, target) \t # 모델 훈련\n",
    "new_observation = [[.5, .5, .5, .5]] \t\t # 새로운 샘플 데이터 생성\n",
    "model.predict(new_observation)  \t\t\t# 클래스 예측\n",
    "model.predict_proba(new_observation)  \n",
    "\n",
    "\n",
    "# MLR 로지스틱 회귀 모델을 만듭니다.\n",
    "logistic_regression = LogisticRegression(random_state=0, multi_class=\"multinomial\")\n",
    "model = logistic_regression.fit(features_standardized, target) \t # 모델 훈련\n",
    "new_observation = [[.5, .5, .5, .5]] \t\t # 새로운 샘플 데이터 생성\n",
    "model.predict(new_observation)  \t\t# 클래스 예측\n",
    "model.predict_proba(new_observation)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################로지스틱 회귀 분류 평가###############################\n",
    "# 타이타닉 데이터로부터 train-test 데이터 분리 (80:20) 생존률 예측\n",
    "(# 분산을 줄이는 하이퍼파라미터 C을 구함)\n",
    "# 로지스틱 회귀 모델을 사용하여 학습\n",
    "#테스트 데이터에 대한 예측 결과 생성후 \n",
    "# 정확도, 정밀도, 재현율 평가 결과 출력\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "raw_data = pd.read_excel('titanic.xls')\n",
    "raw_data.info()\n",
    "\n",
    "# pclass : 객실 등급\n",
    "#survived : 생존 유무\n",
    "#sex : 성별\n",
    "#age : 나이\n",
    "#sibsp : 형제 혹은 부부의 수\n",
    "#parch : 부모, 혹은 자녀의 수\n",
    "#fare : 지불한 운임\n",
    "#boat : 탈출한 보트가 있다면 boat 번호 \n",
    "\n",
    "tmp = []\n",
    "for each in raw_data['sex']:\n",
    "    if each == 'female':\n",
    "        tmp.append(1)\n",
    "    elif each == 'male':\n",
    "        tmp.append(0)\n",
    "    else:\n",
    "        tmp.append(np.nan)\n",
    "\n",
    "raw_data['sex'] = tmp\n",
    "\n",
    "raw_data['survived'] = raw_data['survived'].astype('int')\n",
    "raw_data['pclass'] = raw_data['pclass'].astype('float')\n",
    "raw_data['sex'] = raw_data['sex'].astype('float')\n",
    "raw_data['sibsp'] = raw_data['sibsp'].astype('float')\n",
    "raw_data['parch'] = raw_data['parch'].astype('float')\n",
    "raw_data['fare'] = raw_data['fare'].astype('float')\n",
    "\n",
    "raw_data = raw_data[raw_data['age'].notnull()]\n",
    "raw_data = raw_data[raw_data['sibsp'].notnull()]\n",
    "raw_data = raw_data[raw_data['parch'].notnull()]\n",
    "raw_data = raw_data[raw_data['fare'].notnull()]\n",
    "\n",
    "raw_data.info()\n",
    "features  = raw_data[['pclass','sex','age','sibsp','parch','fare']]\n",
    "features.info()\n",
    "target = raw_data[['survived']].values\n",
    " \n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "scaler = StandardScaler()  \t\t# 특성을 표준화\n",
    "features_standardized = scaler.fit_transform(features )\n",
    "\n",
    "logistic_regression = LogisticRegressionCV( penalty='l2', Cs=1000, random_state=0, n_jobs=-1)\n",
    "model = logistic_regression.fit(features_standardized, target )  # 모델 훈련\n",
    "logistic_regression.C_\n",
    "\n",
    "\n",
    "titanic = raw_data[['pclass','sex','age','sibsp','parch','fare', 'survived']]\n",
    "from sklearn.model_selection import train_test_split\n",
    "#test(valid)/train 을 2:8 로 randomly select\n",
    "train, valid = train_test_split(titanic, test_size=0.2, random_state=0)\n",
    "\n",
    "#train/valid set 완성! \n",
    "train_y=train['survived']\n",
    "train_x=train.drop(['survived'], axis=1)\n",
    "\n",
    "valid_y=valid['survived']\n",
    "valid_x=valid.drop(['survived'],axis=1)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=1000.0, random_state=0 )\n",
    "lr.fit(train_x, train_y)\n",
    "pred_y = lr.predict(valid_x)\n",
    "\n",
    "print(\"Misclassification samples : %d\" %(valid_y != pred_y).sum())\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"accuracy: %.2f\" %accuracy_score(valid_y, pred_y))\n",
    "print(\"Precision : %.3f\" % precision_score(valid_y, pred_y))\n",
    "print(\"Recall : %.3f\" % recall_score(valid_y, pred_y))\n",
    "print(\"F1 : %.3f\" % f1_score(valid_y, pred_y))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()  # 데이터 로드\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "# 결정 트리 분류기 객체 생성 (불순도를 감소를 지표의 기본값을  지니 계수를 사용)\n",
    "decisiontree = DecisionTreeClassifier(random_state=0)\n",
    "model = decisiontree.fit(features, target)  # 모델 훈련\n",
    "\n",
    "observation = [[ 5,  4,  3,  2]]  # New 샘플 데이터\n",
    "model.predict(observation)  # 샘플 데이터의 클래스 예측\n",
    "model.predict_proba(observation)  # 세 개의 클래스에 대한 예측 확률을 확인\n",
    "\n",
    "# 엔트로피를 사용해 결정 트리 분류기를 훈련합니다.\n",
    "decisiontree_entropy = DecisionTreeClassifier( criterion='entropy', random_state=0)\n",
    "model_entropy = decisiontree_entropy.fit(features, target)   # 모델 훈련\n",
    "\n",
    "observation = [[ 5,  4,  3,  2]]  # New 샘플 데이터\n",
    "model_entropy .predict(observation)            # 샘플 데이터의 클래스 예측\n",
    "model_entropy .predict_proba(observation)   # 세 개의 클래스에 대한 예측 확률을 확인\n",
    "\n",
    "\n",
    "#DecisionTree는 분류와 회귀 분석에 모두 사용 가능\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import datasets\n",
    "\n",
    "boston = datasets.load_boston()   # 데이터 로드\n",
    "features = boston.data[:,0:2]   #두 개의 특성만 선택\n",
    "target = boston.target\n",
    "\n",
    "decisiontree = DecisionTreeRegressor(random_state=0)  # 결정 트리 회귀 모델 객체 생성\n",
    "model = decisiontree.fit(features, target)   # 모델 훈련\n",
    "\n",
    "observation = [[0.02, 16]]   #New 샘플 데이터\n",
    "model.predict(observation)   # 샘플 데이터의 타깃을 예측\n",
    "\n",
    "# 평균 제곱 오차를 사용한 (평균 절댓값 오차MAE가 감소되는) 결정 트리 회귀 모델 객체 생성\n",
    "decisiontree_mae = DecisionTreeRegressor(criterion=\"mae\", random_state=0)\n",
    "model_mae = decisiontree_mae.fit(features, target)   # 모델 훈련\n",
    "model_mae.predict(observation)   # 샘플 데이터의 타깃을 예측\n",
    "\n",
    "\n",
    "pip install pyparsing\n",
    "pip install graphviz\n",
    "pip install pydot\n",
    "conda install graphviz\n",
    "\n",
    "\n",
    "1. graphviz.2.38.msi 파일 설치\n",
    "https://graphviz.gitlab.io/_pages/Download/Download_windows.html\n",
    "\n",
    "2. 시스템 환경변수에 추가하기\n",
    "시스템환경변수 > path 추가\n",
    "C:\\Program Files (x86)\\Graphviz2.38\\bin \n",
    "\n",
    "시스템환경변수 > GRAPHVIZ_DOT 새로만들기\n",
    "C:\\Program Files (x86)\\Graphviz2.38\\bin\\dot.exe \n",
    "\n",
    "3. jupyter notebook 재시작\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#confusion_matrix(정답, 예측)\n",
    "confusion_matrix(target,  decisiontree.predict(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############타이타닉 데이터 생존자 분류 분석 (decision tree) ###########\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "df.head()\n",
    "feature_names = [\"pclass\", \"age\", \"sex\"]\n",
    "dfX = df[feature_names ].copy()\n",
    "dfy = df[\"survived\"].copy()\n",
    "dfX.tail()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "dfX[\"sex\"] = LabelEncoder().fit_transform(dfX[\"sex\"])\n",
    "dfX.tail()\n",
    "\n",
    "dfX[\"age\"].fillna(dfX[\"age\"].mean(), inplace=True)\n",
    "dfX.tail()\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "dfX2 = pd.DataFrame(LabelBinarizer().fit_transform(dfX[\"pclass\"]), columns=['c1', 'c2', 'c3'], index=dfX.index)\n",
    "dfX = pd.concat([dfX, dfX2], axis=1)\n",
    "del(dfX['pclass'])\n",
    "dfX.tail()\n",
    "\n",
    "#test, train data 분리 (0.25)\n",
    "#decisiontree 분류 분석\n",
    "#시각화\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import io\n",
    "import pydot \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "from IPython.display import Image\n",
    "from sklearn import tree \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dfX, dfy, test_size=0.25, random_state=0)\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='entropy', max_depth=3, min_samples_leaf=5).fit(X_train, y_train)\n",
    "\n",
    "command_buf = io.StringIO()  #\n",
    "tree.export_graphviz(model, out_file=command_buf, feature_names=[\n",
    "                'Age', 'Sex', '1st_class', '2nd_class', '3rd_class'])\n",
    "graph = pydot.graph_from_dot_data(command_buf.getvalue())[0]\n",
    "image = graph.create_png()\n",
    "Image(image)\n",
    "\n",
    "confusion_matrix(y_train, model.predict(X_train))\n",
    "\n",
    "confusion_matrix(y_test, model.predict(X_test))\n",
    "\n",
    "print(classification_report(y_train, model.predict(X_train)))\n",
    "\n",
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[CPU_ENV]",
   "language": "python",
   "name": "cpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
